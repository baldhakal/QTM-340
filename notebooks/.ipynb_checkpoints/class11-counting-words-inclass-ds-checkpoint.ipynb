{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "*Lauren F. Klein wrote version 1.0 of this notebook, based of tutorials by [Matthew Lavin](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) and [Kavita Ganesan](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XZVlcOdKhSw). I have supplemented it with material from Melanie Walsh's chapter [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF.html) from her online textbook [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/welcome.html).*\n",
    "\n",
    "We will learn powerful data science techniques soon. But, in many cases, just counting words can tell you a lot. \n",
    "\n",
    "Today, we're going to explore a method called Term Frequency - Inverse Document Frequency (tf-idf). Tf-idf comes up a lot in text analysis projects because it’s both a corpus exploration method and a pre-processing step for many other text-mining measures and models.\n",
    "\n",
    "The procedure was introduced in a 1972 paper by Karen Spärck Jones under the name “term specificity,” and the basic idea is this:\n",
    "\n",
    "Instead of representing a term in a document by its raw frequency or its relative frequency (the term count divided by the document length), each term is *weighted* by dividing the term frequency by the number of documents in the corpus containing the word. \n",
    "\n",
    "The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in a document are often the most frequently used words in all of the documents.\n",
    "\n",
    "By contrast, terms with the highest tf-idf scores are the terms in a document that are distinctively frequent in a document when that document is compared other documents. When you sort by tf-idf score, these distinctive terms rise to the top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *New York Times* Obituaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we're going to use tf-idf to study 378 obituaries published by *The New York Times*. This dataset is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). Melanie Walsh re-scraped the obituaries so that the subject's name and death year are included in each text file name; she also added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing: prepare the documents\n",
    "\n",
    "Tf-idf works on a set of documents. Each document needs to be a single string. You'll get very familiar with writing document and text pre-processing code like this by the end of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"../docs/NYT-Obituaries/\"\n",
    "\n",
    "all_docs = []\n",
    "text_titles = []\n",
    "\n",
    "docs = os.listdir(base_dir)\n",
    "\n",
    "for doc in docs:\n",
    "    with open(base_dir + doc, \"r\") as file:\n",
    "        text = file.read()\n",
    "        all_docs.append(text)\n",
    "        text_titles.append(str(doc))\n",
    "# just take a look at the first item to be sure\n",
    "print(docs[0]) \n",
    "print(\"\\n\")\n",
    "print(all_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently scikit-learn, which we were introduced to in the previous lesson, allows us to calculate tf-idf with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd # this will help us keep track of our data; \n",
    "# we'll talk about pandas in more detail later this semester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a doc-term matrix to calculate tf-idf. Remember how to create a doc-term matrix from last lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    "\n",
    "# this steps generates document-term matrix for the docs\n",
    "dtm=cv.fit_transform(all_docs)\n",
    "\n",
    "# check shape\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TfidfTransformer\n",
    "\n",
    "When you initialize TfidfTransformer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf. The recommended way to run `TfidfTransformer` is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call tfidf_transformer.fit on the word count vector we computed earlier.\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce inverse document frequence (idf) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce & print tf-idf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the idf values, you can compute the tf-idf scores for any document or set of documents. Let’s compute tf-idf scores for the documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s print the tf-idf values of the first document to see if they make sense. \n",
    "\n",
    "We'll place the tf-idf scores from the first document (The Who's \"Baba O'Reilly\") into a pandas dataframe and sort the dataframe in descending order of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores for the first doc\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only certain words have scores. This is because only the words in this document have a tf-idf score and everything else, from other documents, shows up as zeroes.\n",
    "\n",
    "Sometimes very common words (\"the\", \"and\", \"a\") are evidently distinctive, but they're not interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf: the fast way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're going to do it again with scikit-learn's stopword list. And since we're tf-idf pros, we're going to use scikit-learn's all-in-one tf-idf vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# to exclude stopwords, add the argument `stop_words='english'`\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', use_idf=True)\n",
    " \n",
    "# send in all your docs here\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as above, get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas dataframe\n",
    "# reminder: we'll cover pandas in a later lesson\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_df = tfidf_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add row for number of times word appears in all documents\n",
    "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at specific words and how they appear in our obituaries corpus. I've entered the eight words below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_slice = tfidf_df[['south', 'war', 'politics', 'peace','america', 'woman', 'music', 'art']]\n",
    "tfidf_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this output make sense? What does it tell you?\n",
    "ANSWER HERE\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own words! In the cell below, replace my eight words with eight of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your results what you expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store tf-idf vectors & print top five for each doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's store our tf-idf vectors to files. Don't worry if you can't follow every bit of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"../docs/NYT-Obituaries/\"\n",
    "\n",
    "# make a directory to store them in\n",
    "os.mkdir(\"./tf_idf_output\")\n",
    "\n",
    "docs = os.listdir(base_dir)\n",
    "\n",
    "csvs = []\n",
    "\n",
    "for doc in docs:\n",
    "    csv = doc.replace(\".txt\",\".csv\")\n",
    "    csvs.append(csv)\n",
    "\n",
    "# convert sparse matrix to array\n",
    "tfidf_vectors_as_array = tfidf_vectorizer_vectors.toarray()\n",
    "\n",
    "# loop each item in tfidf_vectors_as_array, \n",
    "titles = []\n",
    "for counter, doc in enumerate(tfidf_vectors_as_array): # note enumerate. useful! \n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(tfidf_vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    for csv in csvs:\n",
    "        one_doc_as_df.to_csv(path_or_buf=\"./tf_idf_output/\" + str(csv))\n",
    "        title = csv.replace(\".csv\", \"\")\n",
    "        titles.append(title)\n",
    "    print(\"\\n\" + str(titles[counter]) + \" top 5 terms: \")\n",
    "    print(one_doc_as_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the printed results, the top five terms for each song in terms of tf-idf scores. The results are likely obvious for a toy corpus like ours. But does anything surprise you? And what is a corpus, an experiment, that you can imagine using tf-idf for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
