{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with spaCy: \n",
    "## Named Entity Recognition (NER) & Part-of-Speech (POS) Tagging\n",
    "\n",
    "*Lauren F. Klein wrote version 1.0, based off a notebook by Alison Parrish. I have supplemented it with language from Melanie Walsh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NER useful?\n",
    "\n",
    "NER is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters. Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing?\n",
    "\n",
    "Named Entity Recognition is a fundamental task in the field of natural language processing (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called spellcheck? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "We're going to use an open-source Python library, spaCy, for NER and POS tagging today. spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand).\n",
    "\n",
    "The English-language spaCy model that we’re going to use in this lesson was trained on an annotated corpus called “OntoNotes”: 2 million+ words drawn from “news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,” which were meticulously tagged by a group of researchers and professionals for people’s names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n",
    "\n",
    "Okay, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing spaCy\n",
    "\n",
    "From your computer's terminal, type:\n",
    "\n",
    "    pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the spaCy language model \n",
    "\n",
    "Next we need to download the English-language model (en_core_web_sm), which will be processing and making predictions about our texts. This is the model that was trained on the annotated “OntoNotes” corpus. You can download the en_core_web_sm model by running the cell below:\n",
    "\n",
    "    python -m spacy download en_core_web_sm\n",
    "    \n",
    "As above, if you're having trouble with sudo on your machine, just remove the \"sudo\" from the line above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load language model\n",
    "\n",
    "Once the model is downloaded, we need to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We’re going to import spacy and displacy, a special spaCy module for visualization. The `spaCy` code expects all strings to be unicode strings, so make sure you've included `from __future__ import unicode_literals` at the top of your notebook—it'll make your life easier, trust me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process our document with the loaded NLP model. Most of the heavy NLP lifting is done in this line of code.\n",
    "\n",
    "After processing, the document object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
    "\n",
    "Let's create a `Document` object with a few sentences from the Universal Declaration of Human Rights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "\n",
    "spaCy offers an easy way to identify sentences with its `.sents` method. `doc.sents` will turn the doc into a list of sentences. Once you've created a document object, you can iterate over the sentences it contains using the `.sents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the doc: \n",
      "All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.\n",
      "\n",
      "\n",
      "And here are the doc's sentences: \n",
      "All human beings are born free and equal in dignity and rights.\n",
      "They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\n",
      "Everyone has the right to life, liberty and security of person.\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the doc: \")\n",
    "print(doc)\n",
    "print(\"\\n\")\n",
    "print(\"And here are the doc's sentences: \")\n",
    "\n",
    "for item in doc.sents:\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `.sents` attribute is a generator, so you can't index or count it directly. To index or count the .sents attribute, you'll need to convert it to a list first using the `list()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_list = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the number of sentences: 3\n"
     ]
    }
   ],
   "source": [
    "# check the length to make sure it worked\n",
    "\n",
    "print(\"Here's the number of sentences: \" + str(len(sentences_as_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words\n",
    "\n",
    "Iterating over a document yields each word in turn. Words are represented with spaCy [Token](https://spacy.io/docs/api/token) objects, which have several interesting attributes. \n",
    "\n",
    "The `.text` attribute gives the word, and the `.lemma_` attribute gives the word's \"lemma.\"\n",
    "\n",
    "Here are Daniel Jurafsky and James H. Martin on lemmas:\n",
    "\n",
    "\"**Lemmas and Senses**\n",
    "\n",
    "Let’s start by looking at how one word (we’ll choose mouse) might be defined in a dictionary:\n",
    "mouse (N)\n",
    "\n",
    "1.  any of numerous small rodents...\n",
    "2.  a hand-operated device that controls a cursor...\n",
    "\n",
    "Here the form mouse is the `lemma`, also called the citation form. The form mouse would also be the lemma for the word mice; dictionaries don’t have separated efinitions for inflected forms like mice. Similarly sing is the lemma for sing, sang, sung. In many languages the infinitive form is used as the lemma for the verb, so Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The specific forms _sung_ or _carpets_ or _sing_ or _duermes_ are called `wordforms`.\"\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word, lemma\n",
      "\n",
      "All, all\n",
      "human, human\n",
      "beings, being\n",
      "are, be\n",
      "born, bear\n",
      "free, free\n",
      "and, and\n",
      "equal, equal\n",
      "in, in\n",
      "dignity, dignity\n",
      "and, and\n",
      "rights, right\n",
      "., .\n",
      "They, -PRON-\n",
      "are, be\n",
      "endowed, endow\n",
      "with, with\n",
      "reason, reason\n",
      "and, and\n",
      "conscience, conscience\n",
      "and, and\n",
      "should, should\n",
      "act, act\n",
      "towards, towards\n",
      "one, one\n",
      "another, another\n",
      "in, in\n",
      "a, a\n",
      "spirit, spirit\n",
      "of, of\n",
      "brotherhood, brotherhood\n",
      "., .\n",
      "Everyone, everyone\n",
      "has, have\n",
      "the, the\n",
      "right, right\n",
      "to, to\n",
      "life, life\n",
      ",, ,\n",
      "liberty, liberty\n",
      "and, and\n",
      "security, security\n",
      "of, of\n",
      "person, person\n",
      "., .\n"
     ]
    }
   ],
   "source": [
    "print(\"Word, lemma\\n\")\n",
    "for word in doc:\n",
    "    print(word.text + \", \" + word.lemma_)\n",
    "    \n",
    "# Note: On the underscore at the end of a variable, see\n",
    "# https://www.datacamp.com/community/tutorials/role-underscore-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual sentences can also be iterated over to get a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They -PRON-\n",
      "are be\n",
      "endowed endow\n",
      "with with\n",
      "reason reason\n",
      "and and\n",
      "conscience conscience\n",
      "and and\n",
      "should should\n",
      "act act\n",
      "towards towards\n",
      "one one\n",
      "another another\n",
      "in in\n",
      "a a\n",
      "spirit spirit\n",
      "of of\n",
      "brotherhood brotherhood\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "sentence = list(doc.sents)[1]  # same as sentence = sentences_as_list[1]\n",
    "\n",
    "for word in sentence:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy & entities\n",
    "\n",
    "Identifying sentences and words is just the beginning of what spaCy can do! Encoded now in the document is a great deal of information about named entities: that is, about people, places, and many types of things.\n",
    "\n",
    "Let's take a look by loading up one of the songs from our lyrics corpus. Let's try Demi Lovato's \"Made in the USA.\"\n",
    "\n",
    "Remember: **Be sure that you have a folder titled 'lyrics' in the same folder with this notebook on your computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our love runs deep like a Chevy\n",
      "If you fall, I'll fall with you, baby\n",
      "'Cause that's the way we like to do it\n",
      "That's the way we like\n",
      "You run around, open doors like a gentleman\n",
      "Tell me, girl, every day of my everything\n",
      "'Cause that's the way you like to do it\n",
      "That's the way you like\n",
      "\n",
      "Just a little West Coast and a bit of sunshine\n",
      "Hair blowing in the wind, losing track of time\n",
      "Just you and I\n",
      "Just you and I\n",
      "Whoa, whoa\n",
      "\n",
      "No matter how far we go\n",
      "I want the whole world to know I want you bad\n",
      "And I won't have it any other way\n",
      "No matter what the people say\n",
      "I know that we'll never break\n",
      "'Cause our love was made, made in the USA\n",
      "Made in the USA, yeah\n",
      "\n",
      "You always reading my mind like a letter\n",
      "When I'm cold, you're there like a sweater\n",
      "'Cause that's the way we like to do it\n",
      "That's the way we like\n",
      "And never ever let the world get the best of you\n",
      "Every night we're apart, I'm still next to you\n",
      "'Cause that's the way I like to do it\n",
      "That's the way I like\n",
      "\n",
      "We touch down on the East Coast\n",
      "Dinner in the sky rise\n",
      "Winter is the best time for walking in the city lights\n",
      "You and I\n",
      "You and I\n",
      "Whoa, whoa\n",
      "\n",
      "No matter how far we go\n",
      "I want the whole world to know I want you bad\n",
      "And I won't have it any other way\n",
      "No matter what the people say\n",
      "I know that we'll never break\n",
      "'Cause our love was made, made in the USA\n",
      "\n",
      "'Cause, baby, I'll break the bullet\n",
      "And take the blow for love, whoa\n",
      "Our love was made in the USA\n",
      "Made in the USA\n",
      "Made in the USA\n",
      "\n",
      "No matter how far we go\n",
      "I want the whole world to know I want you bad\n",
      "And I won't have it any other way\n",
      "No matter what the people say\n",
      "I know that we'll never break\n",
      "'Cause our love was made, made in the USA\n",
      "Made in the USA, yeah\n",
      "\n",
      "Made in the US\n",
      "Made in the US\n",
      "Made in the USA\n"
     ]
    }
   ],
   "source": [
    "with open(\"./lyrics/Demi-lovato-made-in-the-usa.txt\", \"r\") as file:\n",
    "    lyrics = file.read()\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's turn the lyrics into a spaCy document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|\n",
    "|NORP|Nationalities or religious or political groups.|\n",
    "|FAC|Buildings, airports, highways, bridges, etc.|\n",
    "|ORG|Companies, agencies, institutions, etc.|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|Titles of books, songs, etc.|\n",
    "|LAW|Named documents made into laws.|\n",
    "|LANGUAGE|Any named language.|\n",
    "|DATE|Absolute or relative dates or periods.|\n",
    "|TIME|Times smaller than a day.|\n",
    "|PERCENT|Percentage, including ”%“.|\n",
    "|MONEY|Monetary values, including unit.|\n",
    "|QUANTITY|Measurements, as of weight or distance.|\n",
    "|ORDINAL|“first”, “second”, etc.|\n",
    "|CARDINAL|Numerals that do not fall under another type.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Our love runs deep like a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chevy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</br>If you fall, I'll fall with you, baby</br>'Cause that's the way we like to do it</br>That's the way we like</br>You run around, open doors like a gentleman</br>Tell me, girl, \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    every day\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of my everything</br>'Cause that's the way you like to do it</br>That's the way you like</br></br>Just a little \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    West Coast\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " and a bit of sunshine</br>Hair blowing in the wind, losing track of time</br>Just you and I</br>Just you and I</br>Whoa, whoa</br></br>No matter how far we go</br>I want the whole world to know I want you bad</br>And I won't have it any other way</br>No matter what the people say</br>I know that we'll never break</br>'Cause our love was made, made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", yeah</br></br>You always reading my mind like a letter</br>When I'm cold, you're there like a sweater</br>'Cause that's the way we like to do it</br>That's the way we like</br>And never ever let the world get the best of you</br>Every night we're apart, I'm still next to you</br>'Cause that's the way I like to do it</br>That's the way I like</br></br>We touch down on \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the East Coast\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "</br>Dinner in the sky rise</br>\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Winter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is the best time for walking in the city lights</br>You and I</br>You and I</br>Whoa, whoa</br></br>No matter how far we go</br>I want the whole world to know I want you bad</br>And I won't have it any other way</br>No matter what the people say</br>I know that we'll never break</br>'Cause our love was made, made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br></br>'Cause, baby, I'll break the bullet</br>And take the blow for love, whoa</br>Our love was made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br></br>No matter how far we go</br>I want the whole world to know I want you bad</br>And I won't have it any other way</br>No matter what the people say</br>I know that we'll never break</br>'Cause our love was made, made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", yeah</br></br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br>Made in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc2, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, spaCy can identify \"every day\" and \"Winter\" as `DATE`s, West Coast and East Coast as `LOC`s, and USA as a `GPE`. It recognizes Chevy but thinks its an `ORG`, the company Chevrolet, rather than a `PRODUCT`. This is a good reminder than spaCy is far from perfect and you always need to check how it does!\n",
    "\n",
    "This is all we're going to do with NER today. As you can see this is just the very tip of the iceberg. You could now easily get, say, a full list of persons or orgs or products from any text. Feel free to explore further on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech\n",
    "\n",
    "The `pos_` attribute gives a general part of speech; the `tag_` attribute gives a more specific designation. [List of meanings here.](https://spacy.io/api/annotation#pos-tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word, POS, tag\\n\")\n",
    "\n",
    "for item in doc:\n",
    "    print(item.text, item.pos_, item.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting words by part of speech\n",
    "\n",
    "With knowledge of which part of speech each word belongs to, we can make simple code to extract and recombine words by their part of speech. The following code creates a list of all nouns and adjectives in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for item in doc:\n",
    "    if item.pos_ == 'NOUN':\n",
    "        nouns.append(item.text)\n",
    "for item in doc:\n",
    "    if item.pos_ == 'ADJ':\n",
    "        adjectives.append(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below, some code to print out random pairings of an adjective from the text with a noun from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random.choice(adjectives) + \" \" + random.choice(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a list of verbs works similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for item in doc:\n",
    "    if item.pos_ == 'VERB':\n",
    "        verbs.append(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although in this case, you'll notice the list of verbs is a bit unintuitive. We're getting words like \"should\" and \"are\" and \"has\"—helper verbs that maybe don't fit our idea of what verbs we want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we used the `.pos_` attribute, which only gives us general information about the part of speech. The `.tag_` attribute allows us to be more specific about the kinds of verbs we want. \n",
    "\n",
    "**HOW WOULD WE GET ONLY VERBS IN PAST PARTICPLE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_past = []\n",
    "\n",
    "for item in doc:\n",
    "    # THEN WHAT???\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "only_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger syntactic units\n",
    "\n",
    "So we can get individual words by their part of speech. Great! But what if we want larger chunks, based on their syntactic role in the sentence? The easy way is `.noun_chunks`, which is an attribute of a document or a sentence that evaluates to a list of [spans](https://spacy.io/docs/api/span) of noun phrases, regardless of their position in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in doc.noun_chunks:\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anything more sophisticated than this, though, we'll need to learn about how spaCy parses sentences into its syntactic components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dependency grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![displacy parse](http://static.decontextualize.com/syntax_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a dependency grammar is that every word in a sentence is a \"dependent\" of some other word, which is that word's \"head.\" Those \"head\" words are in turn dependents of other words. The finite verb in the sentence is the ultimate \"head\" of the sentence, and is not itself dependent on any other word. (The dependents of a particular head are sometimes called its \"children.\")\n",
    "\n",
    "The question of how to know what constitutes a \"head\" and a \"dependent\" is complicated. As a starting point, here's a passage from [Dependency Grammar and Dependency Parsing](http://stp.lingfil.uu.se/~nivre/docs/05133.pdf):\n",
    "\n",
    "> Here are some of the criteria that have been proposed for identifying a syntactic relation between a head H and a dependent D in a construction C (Zwicky, 1985; Hudson, 1990):\n",
    ">\n",
    "> 1. H determines the syntactic category of C and can often replace C.\n",
    "> 2. H determines the semantic category of C; D gives semantic specification.\n",
    "> 3. H is obligatory; D may be optional.\n",
    "> 4. H selects D and determines whether D is obligatory or optional.\n",
    "> 5. The form of D depends on H (agreement or government).\n",
    "> 6. The linear position of D is specified with reference to H.\"\n",
    "\n",
    "There are different *types* of relationships between heads and dependents, and each type of relation has its own name. \n",
    "\n",
    "**Use [the displaCy visualizer from earlier](https://demos.explosion.ai/displacy/?text=Everyone%20has%20the%20right%20to%20life%2C%20liberty%20and%20security%20of%20person&model=en&cpu=1&cph=0) to see how a particular sentence is parsed, and what the relations between the heads and dependents are.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of a few dependency relations and what they mean. ([A more complete list can be found here.](http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf))\n",
    "\n",
    "* `nsubj`: this word's head is a verb, and this word is itself the subject of the verb\n",
    "* `nsubjpass`: same as above, but for subjects in sentences in the passive voice\n",
    "* `dobj`: this word's head is a verb, and this word is itself the direct object of the verb\n",
    "* `iobj`: same as above, but indirect object\n",
    "* `aux`: this word's head is a verb, and this word is an \"auxiliary\" verb (like \"have\", \"will\", \"be\")\n",
    "* `attr`: this word's head is a copula (like \"to be\"), and this is the description attributed to the subject of the sentence (e.g., in \"This product is a global brand\", `brand` is dependent on `is` with the `attr` dependency relation)\n",
    "* `det`: this word's head is a noun, and this word is a determiner of that noun (like \"the,\" \"this,\" etc.)\n",
    "* `amod`: this word's head is a noun, and this word is an adjective describing that noun\n",
    "* `prep`: this word is a preposition that modifies its head\n",
    "* `pobj`: this word is a dependent (object) of a preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this works in practice\n",
    "\n",
    "for word in list(doc.sents)[2]:\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print(\"Dependency relation:\", word.dep_)\n",
    "    print(\"Children:\", list(word.children))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using .subtree for extracting syntactic units\n",
    "\n",
    "The `.subtree` attribute evaluates to a generator that can be flatted by passing it to `list()`. This is a list of the word's syntactic dependents—essentially, the \"clause\" that the word belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function merges a subtree and returns a string with the text of the words contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "       return ''.join([w.text_with_ws for w in list(st)]).strip() # just take my word for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function in our toolbox, we can write a loop that prints out the subtree for each word in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(doc.sents)[2]:\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Flattened subtree: \", flatten_subtree(word.subtree))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the subtree and our knowledge of dependency relation types, we can write code that extracts larger syntactic units based on their relationship with the rest of the sentence. For example, to get all of the noun phrases that are subjects of a verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        subjects.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or every prepositional phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases = []\n",
    "for word in doc:\n",
    "    if word.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction\n",
    "\n",
    "A common task in NLP is taking a text and extracting \"named entities\" from it—basically, proper nouns, or names of companies, products, locations, etc. You can easily access this information using the `.ents` property of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"Claire Sterk and I visited the Apple Store in Decatur.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in doc2.ents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity objects have a `.label_` attribute that tells you the type of the entity. ([Here's a full list of the built-in entity types.](https://spacy.io/docs/usage/entity-recognition#entity-types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in doc2.ents:\n",
    "    print(item.text, item.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More on spaCy entity recognition.](https://spacy.io/docs/usage/entity-recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from a file\n",
    "\n",
    "You can load data from a file easily with spaCy. You just have to make sure that the data is in Unicode format, not plain-text. An easy way to do this is to call `.decode('utf8')` on the string after you've loaded it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./2019-09-ccp-corpus-0.3/ccprecords/1850.ME-10.20.PORT.ART.01.txt\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    doc3 = nlp(text) # remember to convert it to a spacy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lo and behold, the named entities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in doc3.ents:\n",
    "    print(item.text, item.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and resources\n",
    "\n",
    "[A few example programs can be found here.](https://github.com/aparrish/rwet-examples/tree/master/spacy)\n",
    "\n",
    "We've barely scratched the surface of what it's possible to do with spaCy. [There's a great page of tutorials on the official site](https://spacy.io/docs/usage/tutorials) that you should check out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
