{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages, metadata\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "meta_df = pd.read_csv('metadata-rh-np.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Column for Classes\n",
    "\n",
    "meta_df['CLASS'] = meta_df['PUBLISHER'] != \"RANDOM HOUSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Texts\n",
    "\n",
    "text_list = []\n",
    "\n",
    "for fpath in meta_df['FILE_PATH']:\n",
    "    with open(fpath, 'r') as file_in:\n",
    "        text = file_in.read()\n",
    "        text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Texts into DTM\n",
    "\n",
    "cv = CountVectorizer()\n",
    "dtm = cv.fit_transform(text_list)\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Processing Functions\n",
    "\n",
    "def top_n_features(dtm_, top_n = 5000):\n",
    "    return dtm_.sum().sort_values()[::-1][:top_n].index.tolist()\n",
    "\n",
    "def normalize(train_dtm_, test_dtm_):\n",
    "    \n",
    "    train_dtm_ = train_dtm_.div(train_dtm_.sum(axis=1), axis = 'rows')\n",
    "    test_dtm_ = test_dtm_.div(test_dtm_.sum(axis=1), axis = 'rows')\n",
    "    \n",
    "    train_mean, train_std = train_dtm_.mean(), train_dtm_.std()\n",
    "    \n",
    "    train_dtm_ = ( train_dtm_ - train_mean ) / train_std\n",
    "    test_dtm_ = ( test_dtm_ - train_mean ) / train_std\n",
    "    \n",
    "    return train_dtm_, test_dtm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Model\n",
    "\n",
    "def author_max(meta_, thresh = 2):\n",
    "    \n",
    "    np_auth_array = meta_[meta_['CLASS']]['AUTHOR'].unique()\n",
    "    rh_auth_array = meta_[~meta_['CLASS']]['AUTHOR'].unique()\n",
    "    \n",
    "    np_sub_index = [meta_[ (meta_['CLASS']) & (meta_['AUTHOR']==auth) ].index.tolist() for auth in np_auth_array]\n",
    "    np_sub_index = [index_ for index_list in np_sub_index\n",
    "                    for index_ in np.random.choice( index_list, min( thresh, len(index_list) ), replace = False ) ]\n",
    "    \n",
    "    rh_sub_index = [meta_[ (~meta_['CLASS']) & (meta_['AUTHOR']==auth) ].index.tolist() for auth in rh_auth_array]\n",
    "    rh_sub_index = [index_ for index_list in rh_sub_index\n",
    "                    for index_ in np.random.choice( index_list, min( thresh, len(index_list) ), replace = False ) ]\n",
    "\n",
    "    return meta_.loc[np_sub_index + rh_sub_index]\n",
    "\n",
    "def bootstrap_set(meta_):\n",
    "    \n",
    "    meta_ = author_max(meta_)\n",
    "    class_size = meta_[meta_['CLASS']].shape[0]\n",
    "    \n",
    "    sample_np_index = meta_[meta_['CLASS']].sample(class_size, replace = True).index.tolist()\n",
    "    sample_np_auths = meta_.loc[sample_np_index]['AUTHOR'].unique()\n",
    "    \n",
    "    sample_rh_index = meta_[~meta_['CLASS']].sample(class_size, replace = True).index.tolist()\n",
    "    sample_rh_auths = meta_.loc[sample_rh_index]['AUTHOR'].unique()\n",
    "    \n",
    "    sample_all_auths = list(sample_np_auths) + list(sample_rh_auths)\n",
    "    \n",
    "    oos_np_index = meta_[( meta_['CLASS']) & (~meta_['AUTHOR'].isin(sample_all_auths))].index.tolist()\n",
    "    oos_rh_index = meta_[(~meta_['CLASS']) & (~meta_['AUTHOR'].isin(sample_all_auths))].index.tolist()\n",
    "    \n",
    "    oos_rh_index = list(np.random.choice(oos_rh_index, size = len(oos_np_index), replace = False))\n",
    "    \n",
    "    return sample_np_index + sample_rh_index, oos_np_index + oos_rh_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From CV, set optimal parameters\n",
    "\n",
    "opt_vocab = 5000\n",
    "opt_reg = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_iters = 2000\n",
    "\n",
    "prediction_by_index = {index_:[] for index_ in meta_df.index.tolist()}\n",
    "f1s = []\n",
    "coefs, coef_count = {}, {}\n",
    "\n",
    "for k in range(boot_iters):\n",
    "    sample_index, oos_index = bootstrap_set(meta_df)\n",
    "    sample_targets, oos_targets = meta_df.loc[sample_index]['CLASS'].astype(int), meta_df.loc[oos_index]['CLASS'].astype(int)\n",
    "    sample_dtm, oos_dtm = dtm_df.loc[sample_index], dtm_df.loc[oos_index]\n",
    "\n",
    "    sample_vocab = top_n_features(sample_dtm, top_n = opt_vocab)\n",
    "    sample_dtm, oos_dtm = sample_dtm[sample_vocab], oos_dtm[sample_vocab]\n",
    "    sample_dtm, oos_dtm = normalize(sample_dtm, oos_dtm)\n",
    "\n",
    "    lr = LogisticRegression(C = opt_reg)\n",
    "    lr.fit(sample_dtm, sample_targets)\n",
    "    oos_predictions = lr.predict_proba(oos_dtm)[:,1]\n",
    "    \n",
    "    f1s.append( f1_score(oos_targets,oos_predictions > 0.5))\n",
    "    \n",
    "    for m,index_ in enumerate(oos_index):\n",
    "        prediction_by_index[index_].append( oos_predictions[m] )\n",
    "    \n",
    "    for n,token in enumerate(sample_vocab):\n",
    "        try:\n",
    "            coef_count[token] += 1\n",
    "            coefs[token][k] = lr.coef_[0][n]\n",
    "        except:\n",
    "            coef_count[token] = 1\n",
    "            coefs[token] = [0] * boot_iters\n",
    "            coefs[token][k] = lr.coef_[0][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "\n",
    "def get_percentile(list_of_floats, pct):\n",
    "    \n",
    "    size = len(list_of_floats)\n",
    "    if size > 0:\n",
    "        list_of_floats = sorted(list_of_floats)\n",
    "        return list_of_floats[int(pct*size)]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Report\n",
    "\n",
    "np.mean(f1s), get_percentile(f1s, 0.025), get_percentile(f1s, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Predictions\n",
    "\n",
    "meta_df['Predicted Probability'] = [np.mean(prediction_by_index[index_]) for index_ in meta_df.index.tolist()]\n",
    "meta_df['Lower (95%-Interval)'] = [get_percentile(prediction_by_index[index_], 0.025) for index_ in meta_df.index.tolist()]\n",
    "meta_df['Upper (95%-Interval)'] = [get_percentile(prediction_by_index[index_], 0.975) for index_ in meta_df.index.tolist()]\n",
    "\n",
    "meta_df.to_csv('baseline-predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame()\n",
    "token_list = [k for k,v in sorted(coef_count.items(), key = lambda item: item[1], reverse = True)[:opt_vocab]]\n",
    "\n",
    "coef_df['Token'] = token_list\n",
    "coef_df['Regression Coefficient'] = [ np.mean(coefs[key_]) for key_ in token_list ]\n",
    "coef_df['Lower (95%-Interval)'] = [ get_percentile(coefs[key_], 0.025) for key_ in token_list ]\n",
    "coef_df['Upper (95%-Interval)'] = [ get_percentile(coefs[key_], 0.975) for key_ in token_list ]\n",
    "\n",
    "coef_df.sort_values('Regression Coefficient', ascending = False, inplace = True)\n",
    "coef_df.to_csv('baseline-model.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
