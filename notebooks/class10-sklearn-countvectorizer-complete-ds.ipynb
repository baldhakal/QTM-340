{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to scikit-learn (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook was inspired by notebooks written by Lauren F. Klein and Melanie Walsh_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of what we'll do the rest of the semester entails turning words into numbers: tf-idf, topic modeling, BERT, similarity, classification, clustering. Python's machine learning library, scikit-learn, will be crucial to many of these methods. Today we'll just introduce ourselves to the library, setting ourselves up for what's to come."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by installing scikit-learn as `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import `CountVectorizer`, which [converts a collection of text documents to a matrix of token counts](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Turning words into numbers in this way allows us to perform analyses according to a philosophy of language called `distributional semantics`, which is at the basis of much of data science with text. Learn more by referring to [today's reading](https://web.stanford.edu/~jurafsky/slp3/6.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in lyrics corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're setting the directory filepath that contains all the lyrics text files that we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The-who-baba-oriley.txt\n",
      "\n",
      "\n",
      "\n",
      "Out here in the fields, I fight for my meals\n",
      "I get my back into my living\n",
      "I don't need to fight to prove I'm right\n",
      "I don't need to be forgiven, yeah, yeah, yeah, yeah, ye-ah\n",
      "\n",
      "\n",
      "Don't cry, don't raise your eye\n",
      "It's only teenage wasteland\n",
      "\n",
      "Sally, take my hand, we'll travel south 'cross land\n",
      "Put out the fire and don't look past my shoulder\n",
      "The exodus is here, the happy ones are near\n",
      "Let's get together before we get much older\n",
      "\n",
      "Teenage wasteland, it's only teenage wasteland\n",
      "Teenage wasteland, oh, yeah\n",
      "Teenage wasteland\n",
      "They're all wasted!\n",
      "\n",
      "[Instrumental]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"./lyrics/\"\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "docs = os.listdir(base_dir)\n",
    "\n",
    "for doc in docs:\n",
    "    with open(base_dir + doc, \"r\") as file:\n",
    "        text = file.read()\n",
    "        all_docs.append(text)\n",
    "\n",
    "# just take a look at the first item to be sure\n",
    "print(docs[0]) \n",
    "print(\"\\n\")\n",
    "print(all_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 2804)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this steps generates document-term matrix for the docs\n",
    "dtm=cv.fit_transform(all_docs)\n",
    "\n",
    "# check shape\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get two numbers `(rows, columns)`. Each `row` is a doc, in this case a song. Each `column` records an element of the total vocabulary for the corpus. Does the result make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128: 1\n",
      "\n",
      "1956: 1\n",
      "\n",
      "1989: 1\n",
      "\n",
      "22nd: 1\n",
      "\n",
      "41: 1\n",
      "\n",
      "441: 1\n",
      "\n",
      "45: 1\n",
      "\n",
      "57: 1\n",
      "\n",
      "abilities: 1\n",
      "\n",
      "able: 1\n",
      "\n",
      "abono: 1\n",
      "\n",
      "about: 42\n",
      "\n",
      "above: 5\n",
      "\n",
      "ac: 1\n",
      "\n",
      "accept: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can look at the vocabulary and counts like this\n",
    "\n",
    "# sum_words is a vector that contains\n",
    "# the sum of each word occurrence in all \n",
    "# texts in the corpus. In other words, \n",
    "# we are adding the elements for each column of\n",
    "# the document-term matrix\n",
    "\n",
    "for x in range(15):\n",
    "    print(str(cv.get_feature_names()[x]) + \": \" + str(dtm.toarray().sum(axis=0)[x]) + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1022\n",
      "you 932\n",
      "to 547\n",
      "and 506\n",
      "it 469\n",
      "me 367\n",
      "on 346\n",
      "my 314\n",
      "that 261\n",
      "we 253\n",
      "in 238\n",
      "yeah 231\n",
      "of 219\n",
      "can 213\n",
      "with 213\n",
      "be 206\n",
      "no 203\n",
      "your 200\n",
      "oh 199\n",
      "got 193\n",
      "just 182\n",
      "baby 179\n",
      "love 178\n",
      "is 172\n",
      "don 167\n",
      "all 165\n",
      "for 163\n",
      "re 148\n",
      "day 135\n",
      "this 129\n"
     ]
    }
   ],
   "source": [
    "# and we can sort it like this:\n",
    "\n",
    "# first we create a dictionary with vocab as keys and counts as values\n",
    "\n",
    "dictVocab = {}\n",
    "for x in range(dtm.shape[1]):\n",
    "    dictVocab[cv.get_feature_names()[x]]=dtm.toarray().sum(axis=0)[x]\n",
    "\n",
    "# then we sort the dictionary in order of counts\n",
    "\n",
    "sortVocab = sorted(dictVocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# then we print top 30\n",
    "\n",
    "for i in sortVocab[0:30]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check 20\n",
      "everybody 20\n",
      "generation 20\n",
      "look 20\n",
      "uh 20\n",
      "alright 19\n",
      "everywhere 19\n",
      "hell 19\n",
      "nothin 19\n",
      "pain 19\n",
      "queen 19\n",
      "radio 19\n",
      "shot 19\n",
      "spark 19\n",
      "truth 19\n",
      "believer 18\n",
      "break 18\n",
      "eyes 18\n",
      "gon 18\n",
      "hair 18\n",
      "keeps 18\n",
      "last 18\n",
      "mutha 18\n",
      "something 18\n",
      "soul 18\n",
      "than 18\n",
      "then 18\n",
      "these 18\n",
      "tonight 18\n",
      "an 17\n"
     ]
    }
   ],
   "source": [
    "# we could also print from a bit lower in the counts\n",
    "\n",
    "for i in sortVocab[200:230]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for today! scikit-learn and CountVectorizer set us up for the rest of the semester..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
