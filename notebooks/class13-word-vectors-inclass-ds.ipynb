{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Vectors ##\n",
    "\n",
    "*This lesson is based on the [Advanced Topics in Word Vectors workshop](https://dh2018.adho.org/en/machine-reading-part-ii-advanced-topics-in-word-vectors/) at DH 2018 as well as tutorials by [Radim Rehurek](https://rare-technologies.com/word2vec-tutorial/) and [Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TK: EXPLAIN WHY WORD VECTORS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/cf/87b25b265d23498b2b70ce873495cf7ef91394c4baff240210e26f3bc18a/gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2MB 20.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/6f/788e657fb513deebadfbb38b346d4878b2fded0f72fe7d937b1646137f46/smart_open-2.1.1.tar.gz (111kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/24/b9facc760789cf844880c178b64d26d9f4a0ef06af3e99473f38fba94657/boto3-1.14.56-py2.py3-none-any.whl (129kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 37.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.18.0,>=1.17.56 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/82/499909b818bddde1a4fc1228389d9d29cc2ede766a2a7370aed033dd07f9/botocore-1.17.56-py2.py3-none-any.whl (6.6MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6MB 26.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.56->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.56->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.1-cp37-none-any.whl size=112415 sha256=c2e84f99dc741f807d39994380e4393f20eb521c2bf29e450edee354baee93a3\n",
      "  Stored in directory: /Users/dsinyki/Library/Caches/pip/wheels/17/49/ea/74939572d8d071ff3c63a98e3e8dadef1117cc93c33efaa504\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.14.56 botocore-1.17.56 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.1\n"
     ]
    }
   ],
   "source": [
    "# gensim is already installed on JupyterHub\n",
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim # remember this! \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, we need to create our corpus.\n",
    "# in this case, we can start by creating the same list of docs as in our previous class\n",
    "\n",
    "import os\n",
    "\n",
    "base_dir = \"../docs/NYT-Obituaries/\" # NOTE: Your path may be different!!!\n",
    "\n",
    "all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# lastly, just check the length of all_docs to see if it's 147\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But here, we also need each doc tokenized by sentence as well. \n",
    "\n",
    "So let's define a function that a takes a list of texts (e.g. our all_docs list) and converts it for gensim word2vec to use. The function will lower-case text and tokenize by sentence and word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need our handy nltk tokenizer \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "        print(len(sentences))  # let's check how many sentences there are per item\n",
    "    return all_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it\n",
    "\n",
    "sentences = make_sentences(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our vectors we call this function below. This function has a couple dozen parameters, some of which are more important than others.\n",
    "Here are a few major ones. The fields that are MANDATORY are marked with an asterisk:\n",
    "1. `sentences*`: This is where you provide your data. It must be in a format of iterable of iterables.\n",
    "2. `sg`: Your choice of training algorithm. There are two standard ways of training W2V vectors -- 'skipgram' and 'CBOW'. If you enter 1 here the skip-gram is applied; otherwise, the default is CBOW.\n",
    "3. `size*`: This is the length of your resulting word vectors. If you have a large corpus (>few billion tokens) you can go up to 100-300 dimensions. Generally word vectors with more dimensions give better results.\n",
    "4. `window`: This is the window of context words you are training on. In other words, how many words come before and after your given word. A good number is 4 here but this can vary depending on what you are interested in. For instance, if you are more interested in embeddings that embody semantic meaning, smaller window sizes work better. \n",
    "5. `alpha`: The learning rate of your model. If you are interested in machine learning experimentation with your vectors you may experiment with this parameter.\n",
    "6. `seed` (int): This is the random seed for your random initialization. All deep learning models initialize the weights with random floats before training. This is a useful field if you want to replicate your experiments because giving this a seed will initialize 'randomly' deterministically.\n",
    "7. `min_count`: This is the minimum frequency threshold. If a given word appears with lower frequency than provided it will be ignored. This is here because words with very low frequency are hard to train.\n",
    "8. `iter`: This is the number of iterations(entire run) over the corpus, also known as epochs. Usually anything between 1-10 is ok. The trade offs are that if you have higher iterations, it will take longer to train and the model may overfit on your dataset. However, longer training will allow your vectors to perform better on tasks relevant to your dataset.\n",
    "\n",
    "Overall, most of these settings wil not concern you unless you are interested in very specific usages of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train our model!\n",
    "\n",
    "ccp_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=2, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at the vocab\n",
    "\n",
    "ccp_model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to save your trained model to disk so that you can reload it as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_model.save('ccp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load a model in the same way (remember this from our topic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = gensim.models.Word2Vec.load('ccp_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing some basic functions\n",
    "\n",
    "# basic similarity\n",
    "ccp_model.wv.most_similar(\"freedom\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity b/t two words\n",
    "\n",
    "print(ccp_model.wv.similarity(w1=\"freedom\",w2=\"justice\"))\n",
    "print(ccp_model.wv.similarity(w1=\"freedom\",w2=\"dinner\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also play with analogy tasks. The commonly seen task is:\n",
    "\n",
    "'Man is to King as Woman is to ____?'\n",
    "\n",
    "\n",
    "' A      is to A\\*.     as B      is to  B\\*  '\n",
    "                         \n",
    "Gensim provides two different ways of implementing this task. You may be more familiar with the the additive version also called the 3CosAdd method:\n",
    "\n",
    "$$\\underset{b*\\in V}{\\textrm{arg max}} (cos(b*,b) - cos(b*,a) + cos(b*,a*))$$\n",
    "\n",
    "This reflects the abstraction of Woman - Man + King. In this maximization, we are searching which word vector will allow us to produce the highest value in this equation.\n",
    "\n",
    "We can implement this method with a provided function. Positive here refers to words that give the positive contribution to similarity (nominator), and negative refers to words that contribute negatively (denominatory). Here's the additive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogies (performed via )\n",
    "# format is: \"freedom is to slavery as liberty is to ???\"\n",
    "\n",
    "ccp_model.wv.most_similar(positive=['liberty', 'slavery'], negative=['freedom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim has quite a few built-in tools, and it's worth taking some time to see what's available. Check the documentation here: [https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's do some visualization ###\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(ccp_model, ['freedom','liberty','slavery','abolition','emancipation'])\n",
    "\n",
    "# display_pca_scatterplot(ccp_model, sample=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
