{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification, Pt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification, a method popular in machine learning, determines whether and how a model can distinguish between sets of text.\n",
    "\n",
    "In the previous lesson, we learned how to:\n",
    "* prepare a dataframe with data and metada for scikit-learn's classifier models\n",
    "* perform leave-one-out classification with a logistic regression model\n",
    "* begin to analyze results and calculate accuracy\n",
    "\n",
    "In this lesson, we will learn how to:\n",
    "* investigate the model's features in terms of p-values and logstic regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quickly review what we learned from last time. For fuller explanations, see the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "As always, we begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from pandas import DataFrame\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import pearsonr, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll return to our corpus of _New York Times_ obituaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect filepaths as files\n",
    "directory = \"../docs/NYT-Obituaries/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and collect obit titles, which are also the final section of the filepaths\n",
    "obit_titles = [Path(file).stem for file in files]\n",
    "obit_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate CountVectorizer as vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('../docs/jockers_stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "# create dtm\n",
    "corpus_path = '../docs/NYT-Obituaries/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make list of filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for title in obit_titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature names and set as column titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "print('df shape is: ' + str(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframee has 378 rows, one for each document, or obituary, and 2985 columns, one for each word that's not in stopwords and appears at least 20 times in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"../docs/NYT-Obituaries.csv\", encoding = 'utf-8')\n",
    "meta = meta.rename(columns={'title': 'obit_title'})\n",
    "meta = meta[[\"obit_title\", \"gender\", \"date\"]]\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our metadata is stored as a pandas dataframe with a row for each obituary and three columns: title, gender, and year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate metadata and doc-term dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([meta, df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize numbers of men and women"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our dataframe to have equal numbers of men and women. How many women are there? Women are counted as 1 and men as 0, so if we sum the gender column, we'll have the number of women:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['gender'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we separate men and women into two dataframes and take a random sample of 93 obituaries about men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_men = df_concat[df_concat['gender'] == 0]\n",
    "df_women = df_concat[df_concat['gender'] == 1]\n",
    "df_men = df_men.sample(n=93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate the sampled men dataframe with the women dataframe and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_men, df_women])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 186 rows: 93 men, 93 women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match meta and data dataframes with subset of df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue to use meta and df, so we need to ensure they match our subsetted df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = df_final[[\"obit_title\", \"gender\", \"date\"]]\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final.loc[:,'000':]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run our classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a dataframe with metadata and vocab counts we're ready to run our classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We add columns for probabilities and predicted class to our metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we run the model, we are going to store its output with our metadata. This will allow us to easily examine the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use scikit-learn's `LogisticRegression` model. There are many other options for classifier models. Some are better for some tasks, other for others. LogisticRegression is standard for classifying literature. We set the penalty as l1 and the 'C' value as 1.0. If you decide to specialize in classification, you can explore further the implications of these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the model in the following for-loop.\n",
    "\n",
    "Classification models need classes: they need the texts grouped into different sets. Our metadata has built-in classes: gender. Men are stored as 0; women as 1. We could, if we wanted, create a new 0/1 class based on year.\n",
    "\n",
    "Each iteration trains on all the titles except one, then predicts which class the excluded title belongs to. We'll call this leave-one-out classification. There are other ways of dividing training and testing sets, which we won't explore today.\n",
    "\n",
    "The first four indented lines simply track our progress by printing index, title, and class. The next four lines exclude a single title, and set the training data and the test data.\n",
    "\n",
    "The final six lines fit the model, calculate the probabilities and predicted class of the test case, and add that information to our metadata dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_index in df_final.index.tolist():\n",
    "    print(this_index) # keep track of where we are in the corpus\n",
    "    title = meta.loc[meta.index[this_index], 'obit_title'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'gender']\n",
    "    print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'gender'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "    print('Class is: ' + str(CLASS) + '\\n' + 'Prediction is: ' + str(predicted) + ' ' + str(prediction) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool is this! For each obituary, we see who it's about, that person's gender (0 or 1), and which gender the model thinks it's about, by which probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we've stored our results in our metadata dataframe. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of those brackets in the PREDICTED column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add a 'RESULT' column that is the result of subtracting the predicted gender from the actual gender.\n",
    "\n",
    "0 means the model was correct.\n",
    "-1 means the model mistook a man for a woman.\n",
    "1 means the model mistook a woman for a man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_column = meta['gender'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the accurate guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_correct = meta[meta['RESULT'] == 0]\n",
    "meta_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many did the model get correct?\n",
    "\n",
    "We can calculate its accuracy by dividing the correct number by the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good rate! At random, the model should guess correctly 50% of the time. It does **much** better than that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-values and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy, its classifications and misclassifications, tells us a lot. \n",
    "\n",
    "But we can learn still much more by exploring the features—the words—that help the model make its classifications.\n",
    "\n",
    "Which words are most likely to tip off the model that a given obituary is about a man or that another is about a woman?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to perform a Z-test and calculate p-values. We'll use these to determine the statistical significance of individual features.\n",
    "\n",
    "**NB**: _As many of you might know, p-values have come under scrutiny in recent years as a measure of significance. The standard threshold for signficance (0.05) is arbitrary. Its meaning is debated. But its authority for a long time incentivized what came to be called p-hacking: the practice of manipulating ones work so it would pass the 0.05 threshold and count as significant. Very recently statisticians and other scholars have argued for abandoning the term \"statistical significance,\" arguing that it reduces the complexity of determining whether a given result is meaningful in context. **tl;dr: don't put too much stock in significance; consider the holistic context of results**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values and logistic regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that takes our metadata and our data as dataframes. It calculates p-values using the Z-test function. It reruns the logistic regression model with all the data. The model makes its predictions by giving each feature, each word, a weight that pulls a text toward the class of man or woman. After running the model, we can draw out the feauture weights, or coefficients, with `clf.coef_[0]`. \n",
    "\n",
    "Then we build a pandas dataframe whose rows are features and whose columns are p-values and logistic regression weights. This function returns that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['gender']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['gender']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['gender']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take it for a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = feat_pval_weight(meta, df)\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see much, here. It'll be easier to see what's happening if we sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more interesting! The negative LR weights tell the model that the obituary is likely about a woman. The positive LR weights tell the model the obituary is likely about a man. \n",
    "\n",
    "We can pull up more data by using the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you infer from the top ten features for each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter further by setting a p-value threshold. For our purposes here, we can set a high threshold, which to normalize we need to divide by the number of features. \n",
    "\n",
    "**Remember** significance is a contested concept. What most matters is understanding the meaning of numbers in context. For us, any feature with a logistic regression weight gives us useful information, and p-values help us understand just how robust that feature is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_thresh = 50.00 / len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter our data by p-values that pass that threshold, sorted by weights. We get rid of features without weights, and separate positive and negative weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feat_df[(feat_df['P_VALUE'] <= sig_thresh)].sort_values('LR_WEIGHT', ascending = True)\n",
    "out = out[out['LR_WEIGHT'] != 0]\n",
    "outM = out[out['LR_WEIGHT'] >= 0]\n",
    "outW = out[out['LR_WEIGHT'] <= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass the remaining features in each dataframe to a list and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outM = outM['FEAT'].tolist()\n",
    "print(\"Here are significant words that distinguish men: \" + str(outM))\n",
    "outW = outW['FEAT'].tolist()\n",
    "print(\"Here are significant words that distinguish women: \" + str(outW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be easier to explore features in a CSV outside of Jupyter. So we write out and move offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.to_csv('../docs/features_obits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're feeling REALLY ambitious, you can use this notebook and the previous one to run classification over your own data. All you need is a text corpus with binary metadata (that is, metadata that can be divided into two classes). Classification works best if you have at least around eighty texts of each class, so our lyrics corpus is too small (though you could try and see what happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
