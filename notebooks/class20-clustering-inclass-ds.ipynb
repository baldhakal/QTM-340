{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "*Based on materials by Brandon Rose, Jacob Eisenstein, and Eun Seo Jo et al.*\n",
    "\n",
    "Last week, we learned about classification, which assumes we have a test set of data that we have already categorized: in our case, we categorized by gender. When we run models with labeled data, we call it _supervised learning_.\n",
    "\n",
    "But what happens when we don't have labeled data? Is it possible to learn anything? \n",
    "\n",
    "This scenario is known as *unsupervised learning*, and we will see that it is very possible to learn about the underlying structure of unlabeled observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "\n",
    "Here’s a scenario: \n",
    "\n",
    "Suppose you scrape thousands of news articles. And suppose you want to organize, or cluster, those documents by their prevalence of language about hurricanes vs elections. You could choose a group of words relate to hurricanes and make the x-axis the frequency with which those words appear in the documents. You could do the same for words related to elections on the y-axis. We might imagine the articles clustering into four groups: one for documents that are largely about a hurricane, another for documents largely about a election, a third for documents about neither, and a fourth for documents, however unlikely, about both.\n",
    "\n",
    "These clusters represent an underlying structure of the data. But it's still supervised, because we've labeled words as belonging to categories and used them to organize documents.\n",
    "\n",
    "Unsupervised learning applies the same basic idea, but in a high-dimensional space with one dimension for every word. As such, the space can’t be visualized. But the goal is the same as in two-dimensions: identify an underlying structure of the observed data, such that documents cluster, and each cluster is internally coherent. \n",
    "\n",
    "**Clustering algorithms** are capable of finding such structure automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter k-means clustering \n",
    "\n",
    "Clustering algorithms assign each data point—for us, each document—to a discrete cluster. One of the best known clustering algorithms is k-means, an iterative algorithm that maintains a cluster assignment for each instance, and a central (\"mean\") location for each cluster. K-means iterates between updates to the assignments and the centers:\n",
    "\n",
    "1. each instance is placed in the cluster with the closest center;\n",
    "\n",
    "2. each center is recomputed as the average over points in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** An important property of k-means is that the converged solution depends on the initialization, and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to it! ##\n",
    "\n",
    "In this example, we're going to use k-means clustering to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list), a corpus created by Brandon Rose. See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the imports. They include old friends: re, BeautifulSoup, nltk, pandas, sklearn, a veritable journey through our semester:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Pre-Proccesing ##\n",
    "\n",
    "Here we go again. Let's pre-process our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import three lists: titles, links, and wikipedia synopses\n",
    "titles = open('title_list.txt').read().split('\\n')\n",
    "\n",
    "# ensures that only the first 100 are read in\n",
    "titles = titles[:100]\n",
    "\n",
    "links = open('link_list_imdb.txt').read().split('\\n')\n",
    "links = links[:100]\n",
    "\n",
    "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_clean_wiki = []\n",
    "\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    # strips html formatting and converts to unicode\n",
    "    synopses_clean_wiki.append(text)\n",
    "\n",
    "synopses_wiki = synopses_clean_wiki\n",
    "    \n",
    "genres = open('genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]\n",
    "\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(links)) + ' links')\n",
    "print(str(len(synopses_wiki)) + ' synopses')\n",
    "print(str(len(genres)) + ' genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get the imdb synopses\n",
    "\n",
    "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    #strips html formatting and converts to unicode\n",
    "    synopses_clean_imdb.append(text)\n",
    "\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the two sets of synopses\n",
    "\n",
    "synopses = []\n",
    "\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)\n",
    "\n",
    "# see what one looks like\n",
    "print(synopses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For document clustering, some people like to stem first ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install textblob # an alternative to spaCy\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now TF-IDF (again!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, \n",
    "                                   tokenizer=textblob_tokenizer, ngram_range=(1,3),\n",
    "                                   min_df=.2, max_df=0.8) #note new params\n",
    "\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(synopses)\n",
    "\n",
    "print(tfidf_vectorizer_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our feature names for future reference \n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first synopsis) just to see what it looks like (we have done this before)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
    "\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And on to the K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our tf-idf vectors, we can now run the k-means clustering algorithm. Remember that K-means initializes with a pre-determined number of clusters. Let's choose 5. \n",
    "\n",
    "**NOTE:** In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following technique:\n",
    "\n",
    "One of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine K.\n",
    "\n",
    "A number of other techniques exist for validating K, but that's the most common one. \n",
    "\n",
    "In any case, back to our clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know \n",
    "\n",
    "km.fit(tfidf_vectorizer_vectors)\n",
    "\n",
    "# km.labels_ gives you the cluster assignments\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump our clusters into a dataframe\n",
    "films = { 'title': titles, 'idx': list(range(100)), 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "film_df = pd.DataFrame(films, columns = ['title', 'idx', 'cluster', 'genre'])\n",
    "\n",
    "film_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many films are in each cluster\n",
    "film_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top terms per cluster\n",
    "\n",
    "# this orders by the distance of each term from the center\n",
    "# (cluster_centers_ returns an array of [n_clusters, n_features] )\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster \" + str(i) + \" top words: \")\n",
    "    top_terms = \"\"\n",
    "   \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        top_terms += terms[ind] + \", \"\n",
    "  \n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all films in each cluster    \n",
    "for i in range(num_clusters):  \n",
    "    print(\"Titles in cluster \" + str(i) + \": \")\n",
    "    cluster_titles = \"\"\n",
    "\n",
    "    # create new df of only the specific cluster\n",
    "    # remember boolean selection! \n",
    "    cluster_df = film_df[ film_df[\"cluster\"] == i ]\n",
    " \n",
    "    # create series of titles assoc w/ that cluster \n",
    "    for title in cluster_df['title']: \n",
    "        cluster_titles += title + \", \"\n",
    "\n",
    "    print(cluster_titles + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 films in each cluster    \n",
    "for i in range(num_clusters):  \n",
    "    \n",
    "    # returns array w/ distances to the centroid i \n",
    "    dist = km.transform(tfidf_vectorizer_vectors)[:, i] \n",
    "    \n",
    "    # return indices for top 10 for that cluster \n",
    "    idxs = np.argsort(dist)[::][:10] \n",
    "    \n",
    "    print(\"Top 10 films in cluster \" + str(i) + \": \")\n",
    "    cluster_top_films = \"\"\n",
    "\n",
    "    # look up film title by index\n",
    "    for idx in idxs: \n",
    "        cluster_top_films += film_df.loc[idx,'title'] + \", \"\n",
    "        \n",
    "    print(cluster_top_films + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with T-SNE\n",
    "\n",
    "You have probably heard of **t-sne** (is it TEA SNEA? or TAE SNAE..)!  This is \"newer\" dimension reduction method that emphasizes visual convenience. Sometimes PCA (which we have used in the past) can produce overlapping/crowding of similar points. The con of tsne is that it is not as easily interpretable as PCA. It's also non-deterministic -- you'll get different but similar results everytime. But I thought you should play with it since it is widely used in machine learning today.\n",
    "\n",
    "Example projects: [hip hop](https://pudding.cool/2017/09/hip-hop-words/) and [wikipedia political ideologies](https://genekogan.com/works/wiki-tSNE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "embed = tsne.fit_transform(tfidf_vectorizer_vectors.toarray())\n",
    "xs, ys = zip(*embed) # we'll use these coords below, after we set up our plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing document clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Cluster 0', \n",
    "                 1: 'Cluster 1', \n",
    "                 2: 'Cluster 2', \n",
    "                 3: 'Cluster 3', \n",
    "                 4: 'Cluster 4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the t-sne plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) # here's where the coords come in \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# iterate through groups to layer the plot\n",
    "# note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return \n",
    "# the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
    "    \n",
    "plt.show() #show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review of cosine similarity with respect to document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is measured against the tf-idf vectors and can be used to generate a measure of \n",
    "similarity between each document and the other documents in the corpus (each synopsis among the synopses). \n",
    "\n",
    "We have seen this before in word2vec's similarity method, but here it is again. For example, here is how you would find the most similar film (in terms of td-idf of synopses) to the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similar_to_0 = cosine_similarity(tfidf_vectorizer_vectors[0:1], tfidf_vectorizer_vectors)\n",
    "\n",
    "# return indices sorted by cosine similarity \n",
    "idxs = np.argsort(-similar_to_0)[::]\n",
    "\n",
    "print(\"Most similar films to \" + film_df.loc[0,'title'] + \":\")\n",
    "       \n",
    "# look up film title by index -- only look at top 10\n",
    "for idx in idxs[0][:10]: \n",
    "    print(film_df.iloc[idx]['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
