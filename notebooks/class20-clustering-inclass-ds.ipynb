{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "*Based on materials by Brandon Rose, Jacob Eisenstein, and Eun Seo Jo et al.*\n",
    "\n",
    "Last week, we learned about classification, which assumes we have a test set of data that we have already categorized: in our case, we categorized by gender. When we run models with labeled data, we call it _supervised learning_.\n",
    "\n",
    "But what happens when we don't have labeled data? Is it possible to learn anything? \n",
    "\n",
    "This scenario is known as *unsupervised learning*, and we will see that it is very possible to learn about the underlying structure of unlabeled observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "\n",
    "Here’s a scenario: \n",
    "\n",
    "Suppose you scrape thousands of news articles. And suppose you want to organize, or cluster, those documents by their prevalence of language about hurricanes vs elections. You could choose a group of words relate to hurricanes and make the x-axis the frequency with which those words appear in the documents. You could do the same for words related to elections on the y-axis. We might imagine the articles clustering into four groups: one for documents that are largely about a hurricane, another for documents largely about a election, a third for documents about neither, and a fourth for documents, however unlikely, about both.\n",
    "\n",
    "These clusters represent an underlying structure of the data. But it's still supervised, because we've labeled words as belonging to categories and used them to organize documents.\n",
    "\n",
    "Unsupervised learning applies the same basic idea, but in a high-dimensional space with one dimension for every word. As such, the space can’t be visualized. But the goal is the same as in two-dimensions: identify an underlying structure of the observed data, such that documents cluster, and each cluster is internally coherent. \n",
    "\n",
    "**Clustering algorithms** are capable of finding such structure automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter k-means clustering \n",
    "\n",
    "Clustering algorithms assign each data point—for us, each document—to a discrete cluster. One of the best known clustering algorithms is k-means, an iterative algorithm that maintains a cluster assignment for each instance, and a central (\"mean\") location for each cluster. K-means iterates between updates to the assignments and the centers:\n",
    "\n",
    "1. each instance is placed in the cluster with the closest center;\n",
    "\n",
    "2. each center is recomputed as the average over points in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** An important property of k-means is that the converged solution depends on the initialization, and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to it! ##\n",
    "\n",
    "In this example, we're going to use k-means clustering to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list), a corpus created by Brandon Rose. See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the imports. They include old friends: re, BeautifulSoup, nltk, pandas, sklearn, a veritable journey through our semester:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Pre-Proccesing ##\n",
    "\n",
    "Here we go again. Let's pre-process our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import three lists: titles, links, and wikipedia synopses\n",
    "titles = open('../docs/title_list.txt').read().split('\\n')\n",
    "\n",
    "# ensures that only the first 100 are read in\n",
    "titles = titles[:100]\n",
    "\n",
    "links = open('../docs/link_list_imdb.txt').read().split('\\n')\n",
    "links = links[:100]\n",
    "\n",
    "synopses_wiki = open('../docs/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_clean_wiki = []\n",
    "\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    # strips html formatting and converts to unicode\n",
    "    synopses_clean_wiki.append(text)\n",
    "\n",
    "synopses_wiki = synopses_clean_wiki\n",
    "    \n",
    "genres = open('../docs/genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]\n",
    "\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(links)) + ' links')\n",
    "print(str(len(synopses_wiki)) + ' synopses')\n",
    "print(str(len(genres)) + ' genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what movies we have and their genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(99):\n",
    "    print(titles[x], genres[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synopses_wiki[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get the imdb synopses\n",
    "\n",
    "synopses_imdb = open('../docs/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    #strips html formatting and converts to unicode\n",
    "    synopses_clean_imdb.append(text)\n",
    "\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the two sets of synopses—from imbdb and wikipedia\n",
    "\n",
    "synopses = []\n",
    "\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)\n",
    "\n",
    "# see what one looks like\n",
    "print(synopses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For document clustering, some people like to stem first ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install textblob # an alternative to spaCy\n",
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can cluster our documents—the movie synopses—we need to prepare them as vectors. We're going to prepare them as *tf-idf* vectors because they provide better clusters than other forms of pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, \n",
    "                                    ngram_range=(1,3), min_df=.2, max_df=0.8) #note new params\n",
    "\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(synopses)\n",
    "\n",
    "print(tfidf_vectorizer_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 100 vectors, one for each synopsis, and each vector has values for 496 features. What are those features? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our feature names for future reference \n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first synopsis) just to see what it looks like \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
    "\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vector for **The Godfather**. It scores high on 'don', 'family', and 'son'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And on to the k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our tf-idf vectors, we can now run the k-means clustering algorithm. Remember that k-means initializes with a pre-determined number of clusters. Let's choose 5. \n",
    "\n",
    "**NOTE:** One of the metrics commonly used to compare results across different values of k is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing k will always decrease this metric, to the extreme of reaching zero when k is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of k is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine the ideal value for k.\n",
    "\n",
    "A number of other techniques exist for validating k, but that's the most common one. \n",
    "\n",
    "In any case, back to our clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know \n",
    "\n",
    "km.fit(tfidf_vectorizer_vectors)\n",
    "\n",
    "# km.labels_ gives you the cluster assignments\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump our clusters into a dataframe\n",
    "films = { 'title': titles, 'idx': list(range(100)), 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "film_df = pd.DataFrame(films, columns = ['title', 'idx', 'cluster', 'genre'])\n",
    "\n",
    "film_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many films are in each cluster\n",
    "film_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top terms per cluster\n",
    "\n",
    "# this orders by the distance of each term from the center\n",
    "# (cluster_centers_ returns an array of [n_clusters, n_features] )\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster \" + str(i) + \" top words: \")\n",
    "    top_terms = \"\"\n",
    "   \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        top_terms += terms[ind] + \", \"\n",
    "  \n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all films in each cluster    \n",
    "for i in range(num_clusters):  \n",
    "    print(\"Titles in cluster \" + str(i) + \": \")\n",
    "    cluster_titles = \"\"\n",
    "\n",
    "    # create new df of only the specific cluster\n",
    "    # remember boolean selection! \n",
    "    cluster_df = film_df[ film_df[\"cluster\"] == i ]\n",
    " \n",
    "    # create series of titles assoc w/ that cluster \n",
    "    for title in cluster_df['title']: \n",
    "        cluster_titles += title + \", \"\n",
    "\n",
    "    print(cluster_titles + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 films in each cluster    \n",
    "for i in range(num_clusters):  \n",
    "    \n",
    "    # returns array w/ distances to the centroid i \n",
    "    dist = km.transform(tfidf_vectorizer_vectors)[:, i] \n",
    "    \n",
    "    # return indices for top 10 for that cluster \n",
    "    idxs = np.argsort(dist)[::][:10] \n",
    "    \n",
    "    print(\"Top 10 films in cluster \" + str(i) + \": \")\n",
    "    cluster_top_films = \"\"\n",
    "\n",
    "    # look up film title by index\n",
    "    for idx in idxs: \n",
    "        cluster_top_films += film_df.loc[idx,'title'] + \", \"\n",
    "        \n",
    "    print(cluster_top_films + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do you think the clustering worked? Why?\n",
    "\n",
    "**Hint: Use the labeled genres as validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *\n",
    "* *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate the pipeline above but replace the movie synopses with our corpus of NYT obituaries or the corpus of lyrics from candidates' playlists.\n",
    "\n",
    "**Hint: you can return to earlier notebooks and copy and paste code to get titles and texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Dimensionality Reduction with T-SNE\n",
    "\n",
    "You have maybe heard of **t-sne** (is it TEA SNEA? or TAE SNAE..).  This is newer dimension reduction method that emphasizes visual convenience. The con of t-sne is that it is not as easily interpretable as k-means. It's also non-deterministic -- you'll get different but similar results everytime. But I thought you should play with it since it is widely used in machine learning today.\n",
    "\n",
    "Example projects: [hip hop](https://pudding.cool/2017/09/hip-hop-words/) and [wikipedia political ideologies](https://genekogan.com/works/wiki-tSNE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "embed = tsne.fit_transform(tfidf_vectorizer_vectors.toarray())\n",
    "xs, ys = zip(*embed) # we'll use these coords below, after we set up our plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing document clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Cluster 0', \n",
    "                 1: 'Cluster 1', \n",
    "                 2: 'Cluster 2', \n",
    "                 3: 'Cluster 3', \n",
    "                 4: 'Cluster 4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the t-sne plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) # here's where the coords come in \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# iterate through groups to layer the plot\n",
    "# note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return \n",
    "# the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
    "    \n",
    "plt.show() #show the plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
